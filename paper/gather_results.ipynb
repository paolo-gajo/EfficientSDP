{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dir = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/paper_results_xavier_uniform/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_name_dict = {\n",
    "    'ade': 'ADE',\n",
    "    'conll04': 'CoNLL04',\n",
    "    'scierc': 'SciERC',\n",
    "    'yamakata': 'ERFGC',\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "# pd.set_option('display.max_rows', 999)\n",
    "def load_results(walk_path: str):\n",
    "    records = []\n",
    "    metric_list = ['tagger_results', 'parser_labeled_results', 'parser_unlabeled_results']\n",
    "    for root, dirs, files in os.walk(walk_path):\n",
    "        for F in files:\n",
    "            filename = os.path.join(root, F)\n",
    "            if filename.endswith('config.json'):                \n",
    "                config_dict = json.load(open(filename))\n",
    "                benchmark_dict = json.load(open(filename.replace('config.json', 'test_results_benchmark.json')))\n",
    "                try:\n",
    "                    params = benchmark_dict['learnable params']\n",
    "                except:\n",
    "                    params = 0\n",
    "                val_results = config_dict['val_results']\n",
    "                test_results = config_dict['test_results']\n",
    "                config = config_dict['config']\n",
    "\n",
    "                for metric in metric_list:\n",
    "                    parser_rnn_type = config['parser_rnn_type']\n",
    "                    par_rnn_h = config['parser_rnn_hidden_size']\n",
    "                    records.append({\n",
    "                        'metric': metric,\n",
    "                        'val_prec': val_results[metric]['P'],\n",
    "                        'val_recall': val_results[metric]['R'],\n",
    "                        'val_f1': val_results[metric]['F1'],\n",
    "                        'test_prec': test_results[metric]['P'],\n",
    "                        'test_recall': test_results[metric]['R'],\n",
    "                        'test_f1': test_results[metric]['F1'],\n",
    "                        'name': config['model_name'],\n",
    "                        'freeze_enc': 'FT' if config['freeze_encoder'] == 0 else 'frozen',\n",
    "                        'params': round(params / 1e6, 2),\n",
    "                        'data': config['dataset_name'],\n",
    "                        'tag_emb_type': config['tag_embedding_type'],\n",
    "                        # 'lora': 'LoRA' if config['use_lora'] == 1 else '/',\n",
    "                        # 'tag_emb': config['use_tag_embeddings_in_parser'],\n",
    "                        'tag_rnn': config['use_tagger_rnn'],\n",
    "                        'par_rnn': parser_rnn_type.upper() if parser_rnn_type != 'none' else '/',\n",
    "                        'par_rnn_l': config['parser_rnn_layers'],\n",
    "                        'mlp_h': config['arc_representation_dim'],\n",
    "                        'arc_norm': config['arc_norm'],\n",
    "                        'par_rnn_h': par_rnn_h if par_rnn_h != 'none' else 0,\n",
    "                        'par_type': config['parser_type'],\n",
    "                        'par_gnn_layers': config['gnn_enc_layers'],\n",
    "                        \n",
    "                        # 'par_res': config['parser_residual'],\n",
    "                        'seed': config['seed'],\n",
    "                        'training_steps': config['training_steps'],\n",
    "                    })\n",
    "    \n",
    "    # Create DataFrame from all collected records at the end\n",
    "    df_aggregate = pd.DataFrame.from_records(records)\n",
    "    # print(df_aggregate.columns)\n",
    "    return df_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new results post May 3rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>val_prec</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>name</th>\n",
       "      <th>freeze_enc</th>\n",
       "      <th>params</th>\n",
       "      <th>data</th>\n",
       "      <th>tag_emb_type</th>\n",
       "      <th>tag_rnn</th>\n",
       "      <th>par_rnn</th>\n",
       "      <th>par_rnn_l</th>\n",
       "      <th>mlp_h</th>\n",
       "      <th>arc_norm</th>\n",
       "      <th>par_rnn_h</th>\n",
       "      <th>par_type</th>\n",
       "      <th>par_gnn_layers</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>1.30</td>\n",
       "      <td>ade</td>\n",
       "      <td>embedding</td>\n",
       "      <td>1</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.1961</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>0.38</td>\n",
       "      <td>ade</td>\n",
       "      <td>embedding</td>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.2211</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0822</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>FT</td>\n",
       "      <td>109.87</td>\n",
       "      <td>ade</td>\n",
       "      <td>embedding</td>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>1.30</td>\n",
       "      <td>ade</td>\n",
       "      <td>embedding</td>\n",
       "      <td>1</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>FT</td>\n",
       "      <td>109.87</td>\n",
       "      <td>ade</td>\n",
       "      <td>embedding</td>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>13.24</td>\n",
       "      <td>yamakata</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>12.82</td>\n",
       "      <td>yamakata</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>13.73</td>\n",
       "      <td>yamakata</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>12.82</td>\n",
       "      <td>yamakata</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>parser_labeled_results</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>frozen</td>\n",
       "      <td>13.73</td>\n",
       "      <td>yamakata</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>simple</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      metric  val_prec  val_recall  val_f1  test_prec  \\\n",
       "1162  parser_labeled_results  0.2132    0.0478      0.0781  0.1717      \n",
       "298   parser_labeled_results  0.2000    0.0470      0.0761  0.1961      \n",
       "1810  parser_labeled_results  0.1873    0.0437      0.0709  0.2211      \n",
       "1153  parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "1801  parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "...                      ...     ...       ...         ...     ...      \n",
       "1627  parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "808   parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "1672  parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "799   parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "1663  parser_labeled_results  0.0000    0.0000      0.0000  0.0000      \n",
       "\n",
       "      test_recall  test_f1               name freeze_enc  params      data  \\\n",
       "1162  0.0409       0.0660   bert-base-uncased  frozen     1.30    ade        \n",
       "298   0.0481       0.0772   bert-base-uncased  frozen     0.38    ade        \n",
       "1810  0.0505       0.0822   bert-base-uncased  FT         109.87  ade        \n",
       "1153  0.0000       0.0000   bert-base-uncased  frozen     1.30    ade        \n",
       "1801  0.0000       0.0000   bert-base-uncased  FT         109.87  ade        \n",
       "...      ...          ...                 ...  ..            ...  ...        \n",
       "1627  0.0000       0.0000   bert-base-uncased  frozen     13.24   yamakata   \n",
       "808   0.0000       0.0000   bert-base-uncased  frozen     12.82   yamakata   \n",
       "1672  0.0000       0.0000   bert-base-uncased  frozen     13.73   yamakata   \n",
       "799   0.0000       0.0000   bert-base-uncased  frozen     12.82   yamakata   \n",
       "1663  0.0000       0.0000   bert-base-uncased  frozen     13.73   yamakata   \n",
       "\n",
       "     tag_emb_type  tag_rnn par_rnn  par_rnn_l  mlp_h  arc_norm  par_rnn_h  \\\n",
       "1162  embedding    1        LSTM    0          100    0         400         \n",
       "298   embedding    0        LSTM    0          100    0         400         \n",
       "1810  embedding    0        LSTM    0          100    0         400         \n",
       "1153  embedding    1        LSTM    0          100    1         400         \n",
       "1801  embedding    0        LSTM    0          100    1         400         \n",
       "...         ...   ..         ...   ..          ...   ..         ...         \n",
       "1627  none         1        LSTM    3          300    1         400         \n",
       "808   none         0        LSTM    3          500    0         400         \n",
       "1672  none         1        LSTM    3          500    0         400         \n",
       "799   none         0        LSTM    3          500    1         400         \n",
       "1663  none         1        LSTM    3          500    1         400         \n",
       "\n",
       "     par_type  par_gnn_layers  seed  training_steps  \n",
       "1162  simple   0               0     10              \n",
       "298   simple   0               0     10              \n",
       "1810  simple   0               0     10              \n",
       "1153  simple   0               0     10              \n",
       "1801  simple   0               0     10              \n",
       "...      ...  ..              ..     ..              \n",
       "1627  simple   0               0     10              \n",
       "808   simple   0               0     10              \n",
       "1672  simple   0               0     10              \n",
       "799   simple   0               0     10              \n",
       "1663  simple   0               0     10              \n",
       "\n",
       "[648 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walk_path = \"/home/pgajo/projects/def-hsajjad/pgajo/Multitask-RFG-torch/results_lstm_steps_10\"\n",
    "df = load_results(walk_path)\n",
    "df = df[df['metric'] == 'parser_labeled_results']\n",
    "df = df.sort_values(by=[\n",
    "    'data',\n",
    "    'tag_emb_type',\n",
    "    'par_rnn_l',\n",
    "    'mlp_h',\n",
    "    'arc_norm',\n",
    "    'test_f1',\n",
    "    ])\n",
    "print(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ablations no tagger rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations'\n",
    "df_no_tagger_load = load_results(walk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_tagger = df_no_tagger_load.drop_duplicates()\n",
    "df_no_tagger = df_no_tagger[df_no_tagger['metric'] == 'parser_labeled_results']\n",
    "# df_no_tagger = df_no_tagger[df_no_tagger['tag_emb'] == '0']\n",
    "# df_no_tagger = df_no_tagger[df_no_tagger['mlp_h'] == '500']\n",
    "# df_no_tagger = df_no_tagger[df_no_tag ger['par_rnn_l'] == '2']\n",
    "# df_no_tagger = df_no_tagger[df_no_tagger['seed'] == '1']\n",
    "# df_no_tagger = df_no_tagger[df_no_tagger['data'] == 'yamakata']\n",
    "\n",
    "# display(df_no_tagger)\n",
    "dataset_list = df_no_tagger['data'].unique()\n",
    "print(f'Samples with no tagger LSTM: {len(df_no_tagger)}')\n",
    "df_no_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ablations w/ tagger rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations_tagger_rnn'\n",
    "df_tagger_load = load_results(walk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagger = df_tagger_load.drop_duplicates()\n",
    "df_tagger = df_tagger[df_tagger['metric'] == 'parser_labeled_results']\n",
    "df_tagger = df_tagger[df_tagger['tag_rnn'] == 1] # we have this here because some experiments with tag_rnn == 1 were mistakenly saved in this folder and then we just re-ran them in the no_tagger dir without removing them from the tagger dir\n",
    "# df_tagger = df_tagger[df_tagger['tag_emb'] == '0']\n",
    "# df_tagger = df_tagger[df_tagger['mlp_h'] == '500']\n",
    "# df_tagger = df_tagger[df_tagger['par_rnn_l'] == '2']\n",
    "# df_tagger = df_tagger[df_tagger['seed'] == '1']\n",
    "# df_tagger = df_tagger[df_tagger['data'] == 'yamakata']\n",
    "\n",
    "# display(df_tagger)\n",
    "dataset_list = df_tagger['data'].unique()\n",
    "print(f'Samples with tagger LSTM: {len(df_tagger)}')\n",
    "df_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x4 charts (top LSTM hidden, top MLP out dim), one fig/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman'],\n",
    "})\n",
    "dataset_list = [\n",
    "    'ade',\n",
    "    'conll04',\n",
    "    'scierc',\n",
    "    'yamakata',\n",
    "]\n",
    "tag_emb_options = [\n",
    "    0,\n",
    "    1,\n",
    "    ]  # sorted(df_filtered['tag_emb'].unique())\n",
    "df_list = [\n",
    "    df_tagger,\n",
    "    df_no_tagger,\n",
    "    ]\n",
    "grouping_cols = [\n",
    "                # 'metric',\n",
    "                'mlp_h',\n",
    "                'par_rnn_l',\n",
    "                'par_rnn_h',\n",
    "                'arc_norm',\n",
    "                'params',\n",
    "                'data',\n",
    "                ]\n",
    "df_counter = 0\n",
    "x_criterion = 'par_rnn_l'\n",
    "color_criterion = 'mlp_h'\n",
    "font_size_big = 60\n",
    "font_size_med = 40\n",
    "\n",
    "overall_best_list = []\n",
    "\n",
    "save_flag = False\n",
    "\n",
    "for df_filtered in df_list:\n",
    "    tagger_rnn_flag = df_filtered['tag_rnn'].unique()\n",
    "    hidden_dims = sorted(df_filtered['par_rnn_h'].unique())[1:]\n",
    "    mlp_out_dims = sorted(df_filtered['mlp_h'].unique())\n",
    "    display(df_filtered)\n",
    "    for tag_emb_opt in tag_emb_options:\n",
    "        df_tag_emb = df_filtered[df_filtered['tag_emb'] == tag_emb_opt]\n",
    "        a = 1\n",
    "        b = 4\n",
    "        f, axs = plt.subplots(a,\n",
    "                            b,\n",
    "                            sharex=True,\n",
    "                            # sharey=True,\n",
    "                            figsize=(b * 10, a * 10))\n",
    "        # f.tight_layout(rect=[0, 0, 1, 1])\n",
    "        # f.suptitle(f'Test F1 vs LSTM layer number. Tagger RNN = {tagger_rnn_flag[0]}. Tag Emb = {tag_emb_opt}.', fontsize = 18)\n",
    "        # f.supxlabel(r'$L_\\psi$', fontsize=font_size_big)       # global x-axis label\n",
    "        # f.supylabel('Test F1 score', fontsize=font_size_big)   # global y-axis label\n",
    "        f.subplots_adjust(bottom=0.20, left=0.06)\n",
    "        df_best_list = []\n",
    "        for i, dataset in enumerate(sorted(dataset_list)):\n",
    "            df_combined_list_hdim = []\n",
    "            best_mean_list_hdim = []\n",
    "            df_data = df_tag_emb[df_tag_emb['data'] == dataset]\n",
    "            # CHECK ACROSS LSTM HIDDEN DIMENSIONS\n",
    "            for h_dim in hidden_dims:\n",
    "                df_grouped_hdim = (df_data[df_data['par_rnn_h'] == h_dim]\n",
    "                    .groupby(grouping_cols)['test_f1']\n",
    "                    .agg(['mean', 'std'])\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'mean': 'test_f1', 'std': 'df_std'})\n",
    "                ).sort_values(x_criterion)\n",
    "\n",
    "                df_zero_hdim = (df_data[df_data['par_rnn_h'] == 0]\n",
    "                    .groupby(grouping_cols)['test_f1']\n",
    "                    .agg(['mean', 'std'])\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'mean': 'test_f1', 'std': 'df_std'})\n",
    "                ).sort_values(x_criterion)\n",
    "\n",
    "                df_combined_hdim = pd.concat([df_zero_hdim, df_grouped_hdim])\n",
    "\n",
    "                df_combined_list_mlp_out = []\n",
    "                best_mean_list_mlp_out = []\n",
    "                # CHECK ACROSS MLP OUTPUT DIMENSIONS\n",
    "                for mlp_out in mlp_out_dims:                    \n",
    "                    df_grouped_mlp_out = df_combined_hdim[df_combined_hdim['mlp_h'] == mlp_out]\n",
    "\n",
    "                    test_f1_norm_mean_mlp_out = df_grouped_mlp_out[(df_grouped_mlp_out['arc_norm'] == 1) & (df_grouped_mlp_out['par_rnn_h'] != 0)]['test_f1'].mean()\n",
    "                    # print(f'test_f1_norm_mean_mlp_out', test_f1_norm_mean_mlp_out)\n",
    "                    test_f1_raw_mean_mlp_out = df_grouped_mlp_out[(df_grouped_mlp_out['arc_norm'] == 0) & (df_grouped_mlp_out['par_rnn_h'] != 0)]['test_f1'].mean()\n",
    "                    # print(f'test_f1_raw_mean_mlp_out', test_f1_raw_mean_mlp_out)\n",
    "                    df_grouped_mlp_out = df_grouped_mlp_out.sort_values(by=['arc_norm', 'mlp_h', 'par_rnn_l', 'par_rnn_h'])\n",
    "                    df_combined_list_mlp_out.append(df_grouped_mlp_out)\n",
    "                    # display(df_grouped_mlp_out)\n",
    "                    best_mean_list_mlp_out.append(max(test_f1_norm_mean_mlp_out, test_f1_raw_mean_mlp_out))\n",
    "                \n",
    "                best_mean_array_mlp_out = np.array(best_mean_list_mlp_out)\n",
    "                best_df_idx_mlp_out = best_mean_array_mlp_out.argmax()\n",
    "                df_best_mlp_out = df_combined_list_mlp_out[best_df_idx_mlp_out]\n",
    "                # display(df_best_mlp_out)\n",
    "\n",
    "                test_f1_norm_mean = df_best_mlp_out[(df_best_mlp_out['arc_norm'] == 1) & (df_best_mlp_out['par_rnn_h'] != 0)]['test_f1'].mean()\n",
    "                # print(f'test_f1_norm_mean', test_f1_norm_mean)\n",
    "                test_f1_raw_mean = df_best_mlp_out[(df_best_mlp_out['arc_norm'] == 0) & (df_best_mlp_out['par_rnn_h'] != 0)]['test_f1'].mean()\n",
    "                # print(f'test_f1_raw_mean', test_f1_raw_mean)\n",
    "                # print(test_f1_norm_mean > test_f1_raw_mean)\n",
    "                df_best_mlp_out = df_best_mlp_out.sort_values(by=['arc_norm', 'mlp_h', 'par_rnn_l', 'par_rnn_h'])\n",
    "                df_combined_list_hdim.append(df_best_mlp_out)\n",
    "                best_mean_list_hdim.append(max(test_f1_norm_mean, test_f1_raw_mean))\n",
    "\n",
    "            # I need to pick the dataframe with the highest mean test f1 (which is always with arc_norm == 1)\n",
    "            best_mean_array = np.array(best_mean_list_hdim)\n",
    "            best_df_idx = best_mean_array.argmax()\n",
    "            df_best = df_combined_list_hdim[best_df_idx]\n",
    "            # reorder columns\n",
    "            df_best_list.append(df_best)\n",
    "            # display(df_best)\n",
    "            data_name = data_name_dict[df_best['data'].unique()[0]]\n",
    "            df_norm_best = df_best[(df_best['arc_norm'] == 1)]\n",
    "            df_raw_best = df_best[(df_best['arc_norm'] == 0)]\n",
    "\n",
    "            x = df_norm_best[x_criterion].astype(int).tolist()\n",
    "            f1_norm_best = df_norm_best['test_f1'].tolist()\n",
    "            f1_raw_best = df_raw_best['test_f1'].tolist()\n",
    "            std_norm_best = df_norm_best['df_std'].tolist()\n",
    "            std_raw_best = df_raw_best['df_std'].tolist()\n",
    "\n",
    "            norm_scaling = 1 if df_norm_best['arc_norm'].unique()[0] == 1 else r'$\\frac{1}{\\sqrt{d}}$'\n",
    "            # print(norm_scaling)\n",
    "            raw_scaling = 1 if df_raw_best['arc_norm'].unique()[0] == 1 else r'$\\frac{1}{\\sqrt{d}}$'\n",
    "            # print(raw_scaling)\n",
    "\n",
    "            par_rnn_h_label = max(df_best['par_rnn_h'].unique())\n",
    "            h_out_label = df_raw_best[color_criterion].unique()[0]\n",
    "            tagger_rnn_flag_latex = r'\\checkmark' if tagger_rnn_flag else r'\\times'\n",
    "            tag_emb_opt_latex = r'\\checkmark' if tag_emb_opt else r'\\times'\n",
    "            figtitle = f\"{data_name} @ \" + r\"$\\psi_h$\" + f\" = {max(df_best['par_rnn_h'].unique())}, \" + \\\n",
    "                    r\"$h_{out}$ = \" + f\"{df_raw_best[color_criterion].unique()[0]} \" + \\\n",
    "                    r\"$\\phi$\" + f\" = {tagger_rnn_flag_latex} \" + \\\n",
    "                    r\"$\\textbf{e}_i^{tag} = \\times$\" + f\" = {tagger_rnn_flag_latex}\"\n",
    "            setting_string = f'tagrnn{tagger_rnn_flag[0]}-tagemb{tag_emb_opt}'\n",
    "            results_dir = os.path.join(paper_dir, setting_string)\n",
    "            if not os.path.exists(results_dir):\n",
    "                os.makedirs(results_dir)\n",
    "            # df_best.to_csv(os.path.join(results_dir, f'{data_name}_{par_rnn_h_label}_{h_out_label}.csv'), float_format = '%.3f')\n",
    "            filename_graph = f'{data_name}-hlstm{par_rnn_h_label}-hout{h_out_label}'\n",
    "            reordered_cols = ['arc_norm', 'par_rnn_l', 'params', 'test_f1', 'df_std',]\n",
    "            df_save = df_best[reordered_cols]\n",
    "            if save_flag:\n",
    "                df_save.to_latex(os.path.join(results_dir, f'{filename_graph}.tex'),\n",
    "                            float_format = '%.3f',\n",
    "                            escape = True,\n",
    "                            index=False,\n",
    "                            caption=f'Results on {figtitle}.',\n",
    "                            label=f'{filename_graph}-{setting_string}',\n",
    "                            )\n",
    "\n",
    "            # Normalized curves with error bars\n",
    "            axs[i].errorbar(x, f1_norm_best, yerr=std_norm_best, fmt='-o', capsize=3, color='#BE4D49', label=f\"a = {norm_scaling}\", linewidth = 3)\n",
    "                        # \\ + \", $|\\Theta|/10^{6} = $ {df_norm_best['params'].tolist()}\")\n",
    "            axs[i].errorbar(x, f1_raw_best, yerr=std_raw_best, fmt='-o', capsize=3, color='#3E7A7C', label=f\"a = {raw_scaling}\", linewidth = 3)\n",
    "                        # \\ + \", $|\\Theta|/10^{6} = $ {df_raw_best['params'].tolist()}\")\n",
    "            # mean_std_norm = [np.array(el).mean() for el in zip(std_norm_best, std_norm_2, std_norm_3)]\n",
    "            # mean_std_raw = [np.array(el).mean() for el in zip(std_raw_best, std_raw_2, std_raw_3)]\n",
    "            # axs[i].plot(x, mean_std_norm, marker='o', linestyle='-', label='mean std (norm)', color='black')\n",
    "            # axs[i].plot(x, mean_std_raw, marker='o', linestyle='--', label='mean std (raw)', color='black')\n",
    "\n",
    "            # axs[i].grid(True)\n",
    "            \n",
    "            # axs[i].set_title(figtitle, fontsize = font_size_big, y = 1.05)\n",
    "            axs[i].set_title(f\"{data_name} @ ({par_rnn_h_label}, {h_out_label})\", fontsize = font_size_big, y = 1.05)\n",
    "            # axs[i].legend(loc='best', fontsize=font_size_med)\n",
    "\n",
    "        grouping_cols_mean = [\n",
    "                # 'metric',\n",
    "                # 'mlp_h',\n",
    "                'par_rnn_l',\n",
    "                # 'par_rnn_h',\n",
    "                'arc_norm',\n",
    "                # 'params',\n",
    "                # 'data',\n",
    "                # 'df_std'\n",
    "                ]\n",
    "        df_mean_data = pd.concat(df_best_list).groupby(by=grouping_cols_mean)['test_f1'].agg(['mean', 'std']).reset_index().rename(columns={'mean': 'test_f1', 'std': 'df_std'}).reset_index()\n",
    "        mean_data_title = f\"{data_name} w/ tagger_rnn_flag = {tagger_rnn_flag[0]}, tag_emb = {tag_emb_opt}\"\n",
    "        print(mean_data_title)\n",
    "        display(df_mean_data)\n",
    "        df_norm_mean_best = df_mean_data[(df_mean_data['arc_norm'] == 1)]\n",
    "        df_raw_mean_best = df_mean_data[(df_mean_data['arc_norm'] == 0)]\n",
    "\n",
    "        x = df_norm_mean_best[x_criterion].astype(int).tolist()\n",
    "        f1_norm_mean_best = df_norm_mean_best['test_f1'].tolist()\n",
    "        print(np.mean(f1_norm_mean_best))\n",
    "        overall_best_list.append(np.mean(f1_norm_mean_best))\n",
    "\n",
    "        f1_raw_mean_best = df_raw_mean_best['test_f1'].tolist()\n",
    "        std_norm_mean_best = df_norm_mean_best['df_std'].tolist()\n",
    "        print(np.mean(f1_raw_mean_best))\n",
    "        std_raw_mean_best = df_raw_mean_best['df_std'].tolist()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # Normalized curves with error bars\n",
    "        norm_scaling = 1 if df_norm_mean_best['arc_norm'].unique()[0] == 1 else r'$\\frac{1}{\\sqrt{d}}$'\n",
    "        # print(norm_scaling)\n",
    "        raw_scaling = 1 if df_raw_mean_best['arc_norm'].unique()[0] == 1 else r'$\\frac{1}{\\sqrt{d}}$'\n",
    "        # print(raw_scaling)\n",
    "        # axs[i].errorbar(x, f1_norm_mean_best, yerr=std_norm_mean_best, fmt='-o', capsize=3, label=f\"a = {norm_scaling}\", color='#BE4D49', linewidth = 3)\n",
    "        # axs[i].errorbar(x, f1_raw_mean_best, yerr=std_raw_mean_best, fmt='-o', capsize=3, label=f\"a = {raw_scaling}\", color='#3E7A7C', linewidth = 3)\n",
    "        # # axs[i].grid(True)\n",
    "        # axs[i].set_title(r\"$\\bigcup_i^4 \\mathcal(D)$\", fontsize = font_size_big, y = 1.05)\n",
    "        # axs[i].legend(loc='best', fontsize=font_size_med)\n",
    "        # integer x‐ticks\n",
    "        # xticks = sorted(df_grouped[x_criterion].astype(int).unique())\n",
    "        # plt.xticks(xticks)\n",
    "        \n",
    "        # plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        # plt.xlabel(x_criterion)\n",
    "        # plt.ylabel(\"Test F1 Score\")\n",
    "        # plt.legend(bbox_to_anchor=(0, 0, 1, 1))\n",
    "        for ax in axs:\n",
    "            ax.tick_params(axis='x', labelsize=font_size_med)\n",
    "            ax.tick_params(axis='y', labelsize=font_size_med+5)\n",
    "        filename = f'{a}x{b}-tagrnn{tagger_rnn_flag[0]}-tagemb{tag_emb_opt}.pdf'\n",
    "        if save_flag:\n",
    "            plt.savefig(os.path.join(paper_dir, filename), format = 'pdf')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "overall_best = np.array(overall_best_list).argmax()\n",
    "overall_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAG RNN ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations_tagger_rnn'\n",
    "df_tagger_load = load_results(walk_path)\n",
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations'\n",
    "df_no_tagger_load = load_results(walk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_tagger = df_no_tagger_load\n",
    "df_no_tagger = df_no_tagger[df_no_tagger['tag_rnn'] == 0]\n",
    "df_no_tagger = df_no_tagger[df_no_tagger['tag_emb'] == 1]\n",
    "df_no_tagger_ablation = df_no_tagger[df_no_tagger['metric'] == 'parser_labeled_results']\n",
    "# print(df_no_tagger_ablation['par_rnn_h'].value_counts())\n",
    "\n",
    "hyperparameter_list = [(300, 100, 'ade'),\n",
    "                       (400, 300, 'conll04'),\n",
    "                       (400, 500, 'scierc'),\n",
    "                       (200, 300, 'yamakata'),\n",
    "                       ]\n",
    "\n",
    "df_no_tagger_ablation_0 = df_no_tagger_ablation[(df_no_tagger_ablation['data'] == hyperparameter_list[0][2]) & (df_no_tagger_ablation['par_rnn_h'] == 0)]\n",
    "df_no_tagger_ablation_0_grouped = df_no_tagger_ablation_0.groupby(by=['data',\n",
    "                                                   'par_rnn_l',\n",
    "                                                   'arc_norm',\n",
    "                                                   'tag_emb',\n",
    "                                                   ])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "# display(df_no_tagger_ablation_0_grouped)\n",
    "\n",
    "df_no_tagger_ablation_no_0 = df_no_tagger_ablation[(df_no_tagger_ablation['data'] == hyperparameter_list[0][2]) & (df_no_tagger_ablation['par_rnn_h'] == hyperparameter_list[0][0]) & (df_no_tagger_ablation['mlp_h'] == hyperparameter_list[0][1])]\n",
    "df_no_tagger_grouped_no_0 = df_no_tagger_ablation_no_0.groupby(by=['data',\n",
    "                                                   'par_rnn_l',\n",
    "                                                   'arc_norm',\n",
    "                                                   'tag_emb',\n",
    "                                                   ])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "# display(df_no_tagger_grouped_no_0)\n",
    "\n",
    "df_no_tagger_grouped = pd.concat([df_no_tagger_ablation_0_grouped, df_no_tagger_grouped_no_0])\n",
    "df_no_tagger_grouped = df_no_tagger_grouped.sort_values(['data', 'par_rnn_l']).round(3)\n",
    "df_no_tagger_grouped['F1'] = df_no_tagger_grouped.apply(lambda row: f\"${row['mean']}\"+r\"\\pm \"+f\"{row['std']}$\", axis= 1)\n",
    "df_no_tagger_grouped = df_no_tagger_grouped.pivot(columns='data',\n",
    "                                            index=['arc_norm',\n",
    "                                                    'par_rnn_l',\n",
    "                                                    # 'tag_emb',\n",
    "                                                    ],\n",
    "                                            values=['F1'],\n",
    "                                            )\n",
    "# df_no_tagger_grouped.to_latex(os.path.join(paper_dir, f'tag_lstm_ablation_results_{hyperparameter_list[0][0]}_{hyperparameter_list[0][1]}.tex'),\n",
    "#                             float_format = '%.3f',\n",
    "#                             escape = True,\n",
    "#                             # index=False,\n",
    "#                             caption=f'Tagger BiLSTM ablation results.',\n",
    "#                             label=f'tab:tag-lstm-ablation-results',\n",
    "#                             )\n",
    "# print(df_no_tagger_grouped)\n",
    "\n",
    "# assume df_no_tagger_ablation already = df_no_tagger[(…filters…)&(metric=='parser_labeled_results')]\n",
    "\n",
    "hyperparameter_list = [\n",
    "    (300, 100, 'ade'),\n",
    "    (400, 300, 'conll04'),\n",
    "    (400, 500, 'scierc'),\n",
    "    (200, 300, 'yamakata'),\n",
    "]\n",
    "\n",
    "all_grouped = []\n",
    "\n",
    "for par_h, mlp_h, dname in hyperparameter_list:\n",
    "    # 1) the “zero‐head” ablation for this dataset\n",
    "    zero = df_no_tagger_ablation[\n",
    "        (df_no_tagger_ablation['data']   == dname) &\n",
    "        (df_no_tagger_ablation['par_rnn_h'] == 0)\n",
    "    ]\n",
    "    g0 = ( zero\n",
    "           .groupby(['arc_norm','par_rnn_l'])['test_f1']\n",
    "           .agg(['mean','std'])\n",
    "           .reset_index()\n",
    "         )\n",
    "    g0['F1'] = g0.apply(lambda r: f\"${r['mean']:.3f}\\\\pm{r['std']:.3f}$\", axis=1)\n",
    "\n",
    "    # 2) the “full‐head” run for this dataset\n",
    "    full = df_no_tagger_ablation[\n",
    "        (df_no_tagger_ablation['data']   == dname) &\n",
    "        (df_no_tagger_ablation['par_rnn_h'] == par_h) &\n",
    "        (df_no_tagger_ablation['mlp_h']     == mlp_h)\n",
    "    ]\n",
    "    g1 = ( full\n",
    "           .groupby(['arc_norm','par_rnn_l'])['test_f1']\n",
    "           .agg(['mean','std'])\n",
    "           .reset_index()\n",
    "         )\n",
    "    g1['F1'] = g1.apply(lambda r: f\"${r['mean']:.3f}\\\\pm{r['std']:.3f}$\", axis=1)\n",
    "\n",
    "    # 3) combine ablation + full, label by dataset\n",
    "    combined = pd.concat([g0, g1], ignore_index=True)\n",
    "    combined['dataset'] = dname\n",
    "    all_grouped.append(combined[['arc_norm','par_rnn_l','dataset','F1']])\n",
    "\n",
    "# 4) make one big long DF, then pivot so each dataset is its own column\n",
    "df_long = pd.concat(all_grouped, ignore_index=True)\n",
    "df_wide = ( df_long\n",
    "            .pivot(index=['arc_norm','par_rnn_l'],\n",
    "                   columns='dataset',\n",
    "                   values='F1')\n",
    "            .sort_index(axis=0)\n",
    "            .sort_index(axis=1)\n",
    "          )\n",
    "\n",
    "# 5) export exactly as you did before\n",
    "outname = f'tag_lstm_ablation_results.tex'\n",
    "df_wide.to_latex(\n",
    "    os.path.join(paper_dir, outname),\n",
    "    escape=True,\n",
    "    caption='Tagger BiLSTM ablation results.',\n",
    "    label='tab:tag-lstm-ablation-results'\n",
    ")\n",
    "\n",
    "# 6) inspect\n",
    "display(df_wide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Emb ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagger = df_tagger_load\n",
    "df_tagger = df_tagger[df_tagger['tag_rnn'] == 0]\n",
    "df_tagger = df_tagger[df_tagger['tag_emb'] == 1]\n",
    "print(len(df_tagger))\n",
    "df_tagger_ablation = df_tagger[df_tagger['metric'] == 'parser_labeled_results']\n",
    "# print(df_tagger_ablation['par_rnn_h'].value_counts())\n",
    "df_tagger_ablation_0 = df_tagger_ablation[(df_tagger_ablation['par_rnn_h'] == 0)]\n",
    "df_tagger_ablation_0_grouped = df_tagger_ablation_0.groupby(by=['data',\n",
    "                                                   'par_rnn_l',\n",
    "                                                   'arc_norm',\n",
    "                                                   'tag_emb',\n",
    "                                                   ])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "# display(df_tagger_ablation_0_grouped)\n",
    "df_tagger_ablation_0 = df_tagger_ablation[(df_tagger_ablation['par_rnn_h'] == 300) & (df_tagger_ablation['mlp_h'] == 100)]\n",
    "df_tagger_grouped_0 = df_tagger_ablation_0.groupby(by=['data',\n",
    "                                                   'par_rnn_l',\n",
    "                                                   'arc_norm',\n",
    "                                                   'tag_emb',\n",
    "                                                   ])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "# display(df_tagger_grouped_0)\n",
    "\n",
    "df_tagger_grouped = pd.concat([df_tagger_ablation_0_grouped, df_tagger_grouped_0])\n",
    "df_tagger_grouped = df_tagger_grouped.sort_values(['data', 'par_rnn_l']).round(3)\n",
    "df_tagger_grouped['F1'] = df_tagger_grouped.apply(lambda row: f\"${row['mean']}\"+r\"\\pm \"+f\"{row['std']}$\", axis= 1)\n",
    "df_tagger_grouped = df_tagger_grouped.pivot(columns='data',\n",
    "                                            index=['arc_norm',\n",
    "                                                    'par_rnn_l',\n",
    "                                                    'tag_emb',\n",
    "                                                    ],\n",
    "                                            values=['F1'],\n",
    "                                            )\n",
    "df_tagger_grouped.to_latex(os.path.join(paper_dir, f'tag_embeddings_ablation_results.tex'),\n",
    "                            float_format = '%.3f',\n",
    "                            escape = True,\n",
    "                            # index=False,\n",
    "                            caption=f'Tag embeddings ablation results.',\n",
    "                            label=f'tab:tag-embeddings-ablation-results',\n",
    "                            )\n",
    "df_tagger_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_results_dir = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_fullft_xu_low_lr'\n",
    "ft_results = load_results(ft_results_dir)\n",
    "print(ft_results['seed'].value_counts())\n",
    "ft_results = ft_results[ft_results['metric'] == 'parser_labeled_results']\n",
    "# display(ft_results[ft_results['data'] == 'scierc'])\n",
    "ft_results_grouped = ft_results.groupby(by=['data',\n",
    "                                            'name',\n",
    "                                            'tag_emb',\n",
    "                                            # 'freeze_enc',\n",
    "                                            'arc_norm'])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "ft_results_grouped = ft_results_grouped.round(3)\n",
    "ft_results_grouped['F1'] = ft_results_grouped.apply(lambda x: f\"{x['mean']} $\\\\pm {x['std']}$\", axis = 1)\n",
    "ft_results_grouped = ft_results_grouped.pivot(\n",
    "    columns=['data'],\n",
    "    index=['arc_norm', 'tag_emb', 'name'],\n",
    "    values=['F1']\n",
    ")\n",
    "ft_results_grouped.to_latex(os.path.join(paper_dir, f'tag_embeddings_ablation_results.tex'),\n",
    "                            float_format = '%.3f',\n",
    "                            escape = True,\n",
    "                            # index=False,\n",
    "                            caption=f'Tag embeddings ablation results.',\n",
    "                            label=f'tab:ft-deberta-results',\n",
    "                            )\n",
    "ft_results_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convergence graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman'],\n",
    "})\n",
    "paper_dir_best = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/paper_results_xavier_uniform'\n",
    "walk_path_convergence = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_best_xu'\n",
    "entries = []\n",
    "dataset_list = ['ade', 'conll04', 'scierc', 'yamakata']\n",
    "for root, dirs, files in os.walk(walk_path_convergence):\n",
    "    for F in files:\n",
    "        filename = os.path.join(root, F)\n",
    "        if filename.endswith('config.json'):\n",
    "            config = json.load(open(os.path.join(root, 'config.json')))\n",
    "            val_results_graph = json.load(open(os.path.join(root, 'val_results.json')))\n",
    "            val_results = config['val_results']\n",
    "            test_results = config['test_results']\n",
    "            entries.append({\n",
    "                'dataset_name': config['config']['dataset_name'],\n",
    "                'arc_norm': config['config']['arc_norm'],\n",
    "                'steps': [el['steps'] for el in val_results_graph],\n",
    "                'f1': [el['parser_labeled_results']['F1'] for el in val_results_graph],\n",
    "            })\n",
    "# df_convergence = pd.DataFrame.from_records(entries)\n",
    "# df_convergence#.groupby(by=['dataset_name', 'arc_norm'])\n",
    "grouped_dict = {}\n",
    "for dataset_name in dataset_list:\n",
    "    for arc_opt in [0, 1]:\n",
    "        grouped_dict[(dataset_name, arc_opt)] = []\n",
    "length_list = []\n",
    "for dataset_name in dataset_list:\n",
    "    for arc_opt in [0, 1]:\n",
    "        for entry in entries:\n",
    "            if entry['dataset_name'] == dataset_name and entry['arc_norm'] == arc_opt:\n",
    "                grouped_dict[(dataset_name, arc_opt)].append(entry['f1'])\n",
    "                length_list.append(len(entry['f1']))\n",
    "\n",
    "grouped_mean = {}\n",
    "grouped_count = {}\n",
    "\n",
    "for key, value_list in grouped_dict.items():\n",
    "    # build nan-padded array\n",
    "    max_len = max(len(s) for s in value_list)\n",
    "    arr = np.full((len(value_list), max_len), np.nan, dtype=float)\n",
    "    for i, series in enumerate(value_list):\n",
    "        arr[i, :len(series)] = series\n",
    "\n",
    "    # mean & count\n",
    "    grouped_mean[key]  = np.nanmean(arr, axis=0)\n",
    "    grouped_count[key] = np.sum(~np.isnan(arr), axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5),\n",
    "                         sharey=True,\n",
    "                        #  sharex=True,\n",
    "                         )\n",
    "colors = ['#3E7A7C', '#BE4D49']\n",
    "for idx, dataset in enumerate(dataset_list):\n",
    "    ax = axes[idx]\n",
    "    for arc_opt in [0, 1]:\n",
    "        mean_s  = grouped_mean[(dataset, arc_opt)]\n",
    "        count_s = grouped_count[(dataset, arc_opt)]\n",
    "        idxs    = np.arange(len(mean_s))\n",
    "        total_runs = count_s.max()\n",
    "\n",
    "        ax.plot(idxs, mean_s, label=f\"arc_norm={arc_opt}\", linewidth = 3, color = colors[arc_opt])\n",
    "        ax.fill_between(\n",
    "            idxs,\n",
    "            mean_s,\n",
    "            where=(count_s < total_runs),\n",
    "            alpha=0.2,\n",
    "            label=f\"Early stopping (arc_norm={arc_opt})\",\n",
    "            color = colors[arc_opt]\n",
    "        )\n",
    "\n",
    "    # ← only every 5th index (i.e. 500 steps) gets a tick\n",
    "    tick_idxs = np.arange(0, len(idxs), 5)\n",
    "    ax.tick_params(axis='both', labelsize=25)\n",
    "    ax.set_xticks(tick_idxs)\n",
    "    ax.set_xticklabels(tick_idxs * 100)\n",
    "\n",
    "    ax.set_title(data_name_dict[dataset], fontsize=35)\n",
    "    # ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(paper_dir_best, 'convergence.pdf'), format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Bhatt setting (original repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/mtrfg_results_og_repo'\n",
    "entries = []\n",
    "metric_list = ['tagger_results', 'parser_labeled_results', 'parser_unlabeled_results',]\n",
    "for root, dirs, files in os.walk(walk_path):\n",
    "    for F in files:\n",
    "        filename = os.path.join(root, F)\n",
    "        if filename.endswith('config.json'):\n",
    "            config = json.load(open(os.path.join(root, 'config.json')))\n",
    "            # print(config)\n",
    "            val_results = json.load(open(os.path.join(root, 'val_results_best.json')))\n",
    "            test_results = json.load(open(os.path.join(root, 'test_results.json')))\n",
    "            for metric in metric_list:\n",
    "                entry = {\n",
    "                \"metric\": metric,\n",
    "                \"arc_norm\": config['arc_norm'],\n",
    "                \"val_p\": val_results[metric]['P'],\n",
    "                \"val_r\": val_results[metric]['R'],\n",
    "                \"val_f1\": val_results[metric]['F1'],\n",
    "                \"test_p\": test_results[metric]['P'],\n",
    "                \"test_r\": test_results[metric]['R'],\n",
    "                \"test_f1\": test_results[metric]['F1'],}\n",
    "                entries.append(entry)\n",
    "df_bhatt_og = pd.DataFrame.from_records(entries)\n",
    "df_bhatt_og_group_p = df_bhatt_og.groupby(by=['metric', 'arc_norm'])['test_p'].agg(['mean', 'std']).reset_index()\n",
    "df_bhatt_og_group_p = df_bhatt_og_group_p.round(3)\n",
    "\n",
    "df_bhatt_og_group_p['combined'] = df_bhatt_og_group_p.apply(lambda row: f\"{row['mean']} pm {row['std']}\", axis= 1)\n",
    "df_bhatt_og_group_p = df_bhatt_og_group_p.pivot(index='arc_norm', columns='metric', values=['combined'])\n",
    "print('precision')\n",
    "display(df_bhatt_og_group_p)\n",
    "df_bhatt_og_group_r = df_bhatt_og.groupby(by=['metric', 'arc_norm'])['test_r'].agg(['mean', 'std']).reset_index()\n",
    "df_bhatt_og_group_r = df_bhatt_og_group_r.round(3)\n",
    "\n",
    "df_bhatt_og_group_r['combined'] = df_bhatt_og_group_r.apply(lambda row: f\"{row['mean']} pm {row['std']}\", axis= 1)\n",
    "df_bhatt_og_group_r = df_bhatt_og_group_r.pivot(index='arc_norm', columns='metric', values=['combined'])\n",
    "print('recall')\n",
    "display(df_bhatt_og_group_r)\n",
    "df_bhatt_og_group_f1 = df_bhatt_og.groupby(by=['metric', 'arc_norm'])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "df_bhatt_og_group_f1 = df_bhatt_og_group_f1.round(3)\n",
    "\n",
    "df_bhatt_og_group_f1['combined'] = df_bhatt_og_group_f1.apply(lambda row: f\"{row['mean']} pm {row['std']}\", axis= 1)\n",
    "df_bhatt_og_group_f1 = df_bhatt_og_group_f1.pivot(index='arc_norm', columns='metric', values=['combined'])\n",
    "print('f1')\n",
    "display(df_bhatt_og_group_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get Bhatt setting (updated repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bhatt = df_tagger\n",
    "# df_bhatt['name'] = 'bert'\n",
    "df_bhatt = df_bhatt[df_bhatt['par_rnn_l'] == 3]\n",
    "df_bhatt = df_bhatt[df_bhatt['par_rnn_h'] == 400]\n",
    "df_bhatt = df_bhatt[df_bhatt['mlp_h'] == 500]\n",
    "df_bhatt = df_bhatt[df_bhatt['tag_emb'] == 1]\n",
    "# df_bhatt = df_bhatt[df_bhatt['arc_norm'] == 0]\n",
    "df_bhatt = df_bhatt.groupby(by=['data', 'arc_norm'])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "print(len(df_bhatt))\n",
    "df_bhatt = df_bhatt.round(3)\n",
    "display(df_bhatt)\n",
    "df_bhatt['F1'] = df_bhatt.apply(lambda row: f\"${row['mean']}\"+r\"\\pm \"+f\"{row['std']}$\", axis= 1)\n",
    "df_bhatt = df_bhatt.drop(['mean', 'std'], axis = 1)\n",
    "df_bhatt.to_latex(os.path.join(paper_dir, f'bhatt_results.tex'),\n",
    "                            float_format = '%.3f',\n",
    "                            escape = True,\n",
    "                            index=False,\n",
    "                            caption=f'Bhatt results.',\n",
    "                            label=f'tab:bhatt-results',\n",
    "                            )\n",
    "display(df_bhatt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_steps_lora_ft'\n",
    "df_aggregate_ft = load_results(walk_path)\n",
    "dataset_list = [\n",
    "    'ade',\n",
    "    'conll04',\n",
    "    'scierc',\n",
    "    'yamakata'\n",
    "    ]\n",
    "\n",
    "# for dataset in dataset_list:\n",
    "    # df_aggregate_ft = df_aggregate_ft[df_aggregate_ft['data'] == dataset]\n",
    "df_aggregate_ft = df_aggregate_ft[df_aggregate_ft['metric'] == 'parser_labeled_results']\n",
    "df_aggregate_ft = df_aggregate_ft.sort_values(by='test_f1', ascending=False)\n",
    "# df_aggregate_ft = df_aggregate_ft[df_aggregate_ft['par_rnn_l'] == 3]\n",
    "\n",
    "# degrees of freedom\n",
    "# print(df_aggregate_ft['tag_emb'].value_counts())\n",
    "# print(df_aggregate_ft['arc_norm'].value_counts())\n",
    "# print(df_aggregate_ft['freeze_enc'].value_counts())\n",
    "\n",
    "display(df_aggregate_ft)\n",
    "\n",
    "df_group = (\n",
    "    df_aggregate_ft\n",
    "    .groupby(['metric',\n",
    "            'arc_norm',\n",
    "            'tag_emb',\n",
    "            # 'params',\n",
    "            'data',\n",
    "            'lora',\n",
    "            ])['test_f1']\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'test_f1', 'std': 'df_std'})\n",
    ")\n",
    "\n",
    "df_group = df_group.sort_values(by=['tag_emb', 'arc_norm', 'data',])\n",
    "\n",
    "# df_group = df_group[df_group['par_rnn_l'] == 3]\n",
    "# df_group = df_group[df_group['arc_norm'] == 0]\n",
    "\n",
    "df_group.to_latex(os.path.join(paper_dir, f'ft_lora_results.tex'),\n",
    "                            float_format = '%.3f',\n",
    "                            escape = True,\n",
    "                            index=False,\n",
    "                            caption=f'Full fine-tuning and LoRA results.',\n",
    "                            label=f'tab:full-ft-lora-results',\n",
    "                            )\n",
    "display(df_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/resultstagger_gnn_xu'\n",
    "df_aggregate_gnn = load_results(walk_path)\n",
    "df_aggregate_gnn = df_aggregate_gnn[df_aggregate_gnn['metric'] == 'parser_labeled_results']\n",
    "df_aggregate_gnn = df_aggregate_gnn[df_aggregate_gnn['data'] == 'yamakata']\n",
    "# print(df_aggregate_gnn['freeze_enc'].describe())\n",
    "# print(df_aggregate_gnn['seed'].value_counts())\n",
    "display(df_aggregate_gnn)\n",
    "df_group_gnn = df_aggregate_gnn.groupby(by = ['data',\n",
    "                                              'par_rnn_l',\n",
    "                                              'par_rnn_h',\n",
    "                                              'mlp_h',\n",
    "                                              'par_gnn_layers',\n",
    "                                              'arc_norm'\n",
    "                                              ])['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "df_group_gnn.sort_values(['par_rnn_l', 'par_gnn_layers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM complete results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations'\n",
    "df_no_tagger = load_results(walk_path)\n",
    "display(df_no_tagger)\n",
    "walk_path = '/home/pgajo/projects/def-hsajjad/pgajo/mtrfg_results/results_lstm_size_ablations_tagger_rnn'\n",
    "df_tagger = load_results(walk_path)\n",
    "df_tagger = df_tagger[df_tagger['tag_rnn'] == 1]\n",
    "display(df_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppp(x):\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "tag_emb_opts = (0, 1)\n",
    "df_list = [df_tagger]\n",
    "for df in df_list:\n",
    "    for tag_emb_option in tag_emb_opts:\n",
    "        df_no_tagger_filtered = df[df['tag_emb'] == tag_emb_option]\n",
    "        # display(df_no_tagger_filtered)\n",
    "        # df_no_tagger_group_data = df_no_tagger_filtered.groupby(by=[\n",
    "        #                         'metric',\n",
    "        #                         # 'data',\n",
    "        #                         'tag_emb',\n",
    "        #                         'par_rnn_l',\n",
    "        #                         'par_rnn_h',\n",
    "        #                         'mlp_h',\n",
    "        #                         'arc_norm',\n",
    "        #                         ],)['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "        # df_no_tagger_group_data = df_no_tagger_group_data.sort_values('mean')\n",
    "        # display(df_no_tagger_group_data)\n",
    "        df_no_tagger_group = df_no_tagger_filtered.groupby(by=[\n",
    "                                'metric',\n",
    "                                'data',\n",
    "                                'tag_emb',\n",
    "                                'par_rnn_l',\n",
    "                                'par_rnn_h',\n",
    "                                'mlp_h',\n",
    "                                'arc_norm',\n",
    "                                ],)['test_f1'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "        df_no_tagger_group['mean_std'] = df_no_tagger_group.apply(\n",
    "                lambda row: f\"{row['mean']:.3f} ± {row['std']:.3f}\", axis=1\n",
    "            )\n",
    "        # df_no_tagger_group = df_no_tagger_group.drop(['mean', 'std'], axis=1)\n",
    "        df_no_tagger_group\n",
    "\n",
    "        tag_emb_flag = df_no_tagger_filtered['tag_emb'].unique()[0]\n",
    "        tag_rnn_flag = df_no_tagger_filtered['tag_rnn'].unique()[0]\n",
    "\n",
    "        pivot_df = df_no_tagger_group.pivot(\n",
    "            values=[\n",
    "                    # 'mean_std',\n",
    "                    'mean',\n",
    "                    # 'std',\n",
    "                    ],\n",
    "            index=['arc_norm',\n",
    "                    'par_rnn_l',\n",
    "                    'metric',\n",
    "                    'tag_emb',\n",
    "                    'par_rnn_h',\n",
    "                    'mlp_h',\n",
    "                    ],\n",
    "            columns='data'\n",
    "        ).reset_index()\n",
    "        display(pivot_df)\n",
    "        \n",
    "        # pivot_df['overall_mean'] = pivot_df.apply(lambda row: ppp(row))\n",
    "        \n",
    "        # pivot_df.to_latex(os.path.join(paper_dir, f'lstm_results_complete_tagemb-{tag_emb_flag}_tagrnn-{tag_rnn_flag}.tex'),\n",
    "        #                             float_format='%.3f',\n",
    "        #                             escape=True,\n",
    "        #                             # index=False,\n",
    "        #                             caption=f'Complete LSTM results - tagemb-{tag_emb_flag} - tagrnn-{tag_rnn_flag}.',\n",
    "        #                             label=f\"tab:lstm-results-complete_tagemb-{tag_emb_flag}_tagrnn-{tag_rnn_flag}\",\n",
    "        #                             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
